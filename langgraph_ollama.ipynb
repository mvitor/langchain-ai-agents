{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f165054e-935a-4561-b9d3-13750514102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "from trustcall import create_extractor\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3507d8da-f4fa-460c-bc75-531d31ea3515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class UserProfile(TypedDict):\n",
    "    \"\"\"User profile schema with typed fields\"\"\"\n",
    "    user_name: str  # The user's preferred name\n",
    "    interests: List[str]  # A list of the user's interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d102a92b-06f0-4213-b201-dd99aa4514b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Marcio',\n",
       " 'interests': ['ai', 'technology', 'malta', 'speak english']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TypedDict instance\n",
    "user_profile: UserProfile = {\n",
    "    \"user_name\": \"Marcio\",\n",
    "    \"interests\": [\"ai\", \"technology\", \"malta\", \"speak english\"]\n",
    "}\n",
    "user_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34e24ac2-2b78-4db6-bc21-b82b7e04bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the in-memory store\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memory\")\n",
    "\n",
    "# Save a memory to namespace as key and value\n",
    "key = \"user_profile\"\n",
    "value = user_profile\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6d7b7d4-72e8-497c-936c-f6cacaf27038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_name': 'Marcio',\n",
       " 'interests': ['ai', 'technology', 'malta', 'speak english']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the memory by namespace and key\n",
    "profile = in_memory_store.get(namespace_for_memory, \"user_profile\")\n",
    "profile.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257d2d8-62b5-4e29-9a51-922bacde4371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"llama3.1:8b\", api_key=\"ollama\",  temperature=0.7,   base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "\n",
    "# Schema \n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\" Profile of a user \"\"\"\n",
    "    user_name: str = Field(description=\"The user's preferred name\")\n",
    "    user_location: str = Field(description=\"The user's location\")\n",
    "    interests: list = Field(description=\"A list of the user's interests\")\n",
    "\n",
    "# Create the extractor\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[UserProfile],\n",
    "    tool_choice=\"UserProfile\", # Enforces use of the UserProfile tool\n",
    ")\n",
    "\n",
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "# Extraction instruction\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Create or update the memory (JSON doc) to incorporate information from the following conversation:\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # Format the memories for the system prompt\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Location: {memory_dict.get('user_location', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\"      \n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    # Format the memory in the system prompt\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "\n",
    "    # Respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve existing memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "        \n",
    "    # Get the profile as the value from the list, and convert it to a JSON doc\n",
    "    existing_profile = {\"UserProfile\": existing_memory.value} if existing_memory else None\n",
    "    print(\"existing_profile\")\n",
    "    print(existing_profile)\n",
    "    print(\"state\")\n",
    "    print(state)\n",
    "    # Invoke the extractor\n",
    "    result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=TRUSTCALL_INSTRUCTION)]+state[\"messages\"], \"existing\": existing_profile})\n",
    "    print(\"result\")\n",
    "    print(result)    \n",
    "    # Get the updated profile as a JSON object\n",
    "    updated_profile = result[\"responses\"][0].model_dump()\n",
    "\n",
    "    # Save the updated profile\n",
    "    key = \"user_memory\"\n",
    "    store.put(namespace, key, updated_profile)\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Store for long-term (across-thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# Checkpointer for short-term (within-thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph with the checkpointer fir and store\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# View\n",
    "#display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fa017ca-b93e-4b08-8756-ecbfce768434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, my name is Marcio. I live in Malta and i love AI\n",
      "Skipping chunk with no messages or empty messages list.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Marcio! It's great to meet you. Living in Malta sounds wonderful, and it's fantastic that you love AI. If there's anything specific you'd like to discuss or learn about AI, feel free to ask!\n",
      "Skipping chunk with no messages or empty messages list.\n",
      "existing_profile\n",
      "None\n",
      "state\n",
      "{'messages': [HumanMessage(content='Hi, my name is Marcio. I live in Malta and i love AI', additional_kwargs={}, response_metadata={}, id='de4b6e89-1577-402d-9b08-5696b630dc44'), AIMessage(content=\"Hi Marcio! It's great to meet you. Living in Malta sounds wonderful, and it's fantastic that you love AI. If there's anything specific you'd like to discuss or learn about AI, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 68, 'total_tokens': 111, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_8fd43718b3', 'id': 'chatcmpl-BPqBju1TCgh8cnFhceDjbYrbhuMYd', 'finish_reason': 'stop', 'logprobs': None}, id='run-30c6d41a-49c3-4adf-9b2c-480dc00c034f-0', usage_metadata={'input_tokens': 68, 'output_tokens': 43, 'total_tokens': 111, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "result\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_wx1izkfDG7c6YkrFyperaX22', 'function': {'arguments': '{\"user_name\":\"Marcio\",\"user_location\":\"Malta\",\"interests\":[\"AI\"]}', 'name': 'UserProfile'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 179, 'total_tokens': 199, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_eede8f0d45', 'id': 'chatcmpl-BPqBjXJSvjdXwzY1kJh1CXJyVojpY', 'finish_reason': 'stop', 'logprobs': None}, id='run-8372186f-d9b9-4e9b-9dc1-f4b9b732886e-0', tool_calls=[{'name': 'UserProfile', 'args': {'user_name': 'Marcio', 'user_location': 'Malta', 'interests': ['AI']}, 'id': 'call_wx1izkfDG7c6YkrFyperaX22', 'type': 'tool_call'}], usage_metadata={'input_tokens': 179, 'output_tokens': 20, 'total_tokens': 199, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})], 'responses': [UserProfile(user_name='Marcio', user_location='Malta', interests=['AI'])], 'response_metadata': [{'id': 'call_wx1izkfDG7c6YkrFyperaX22'}], 'attempts': 1}\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Marcio. I live in Malta and i love AI\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    # Check if the 'messages' key exists and the list is not empty\n",
    "    if \"messages\" in chunk and chunk[\"messages\"]:\n",
    "        # Now it's safe to access the last element\n",
    "        last_message = chunk[\"messages\"][-1]\n",
    "        # Check if the last message has the pretty_print method (good practice)\n",
    "        if hasattr(last_message, 'pretty_print') and callable(last_message.pretty_print):\n",
    "             last_message.pretty_print()\n",
    "        else:\n",
    "             print(f\"Last message: {last_message}\") # Fallback if no pretty_print\n",
    "    # else:\n",
    "        # Optional: print something if messages are empty or missing for debugging\n",
    "        print(\"Skipping chunk with no messages or empty messages list.\")\n",
    "        # print(f\"Current chunk keys: {chunk.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9da9ee42-2fa2-4471-ab86-87d89d13fe82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I like to study AI and pratice english in my free time\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds like a great way to spend your free time, Marcio! Studying AI can be really exciting, and practicing English will definitely help you communicate even better. If you ever want to discuss AI topics or practice your English skills, I'm here to help. Do you have any particular AI areas you're interested in or specific topics you'd like to explore?\n",
      "existing_profile\n",
      "{'UserProfile': {'user_name': 'Marcio', 'user_location': 'Malta', 'interests': ['AI']}}\n",
      "state\n",
      "{'messages': [HumanMessage(content='Hi, my name is Marcio. I live in Malta and i love AI', additional_kwargs={}, response_metadata={}, id='de4b6e89-1577-402d-9b08-5696b630dc44'), AIMessage(content=\"Hi Marcio! It's great to meet you. Living in Malta sounds wonderful, and it's fantastic that you love AI. If there's anything specific you'd like to discuss or learn about AI, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 68, 'total_tokens': 111, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_8fd43718b3', 'id': 'chatcmpl-BPqBju1TCgh8cnFhceDjbYrbhuMYd', 'finish_reason': 'stop', 'logprobs': None}, id='run-30c6d41a-49c3-4adf-9b2c-480dc00c034f-0', usage_metadata={'input_tokens': 68, 'output_tokens': 43, 'total_tokens': 111, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='I like to study AI and pratice english in my free time', additional_kwargs={}, response_metadata={}, id='0c22efd0-d0e7-4ed1-ac36-bcfccbad1da5'), AIMessage(content=\"That sounds like a great way to spend your free time, Marcio! Studying AI can be really exciting, and practicing English will definitely help you communicate even better. If you ever want to discuss AI topics or practice your English skills, I'm here to help. Do you have any particular AI areas you're interested in or specific topics you'd like to explore?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 143, 'total_tokens': 215, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_8fd43718b3', 'id': 'chatcmpl-BPqDhBKTwM0Dg9chGd7Xs5kU5Awxj', 'finish_reason': 'stop', 'logprobs': None}, id='run-fb94d978-56a9-4d99-94e4-b10f34ec2a12-0', usage_metadata={'input_tokens': 143, 'output_tokens': 72, 'total_tokens': 215, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "result\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'updated_docs': {'call_umGySnuvUTY4fJ9SY2iVLLC7': 'UserProfile'}}, response_metadata={}, id='d353dd00-4398-4dd4-b615-61b428a2fdd4', tool_calls=[{'name': 'UserProfile', 'args': {'user_name': 'Marcio', 'user_location': 'Malta', 'interests': ['AI', 'English practice']}, 'id': 'call_umGySnuvUTY4fJ9SY2iVLLC7', 'type': 'tool_call'}])], 'responses': [UserProfile(user_name='Marcio', user_location='Malta', interests=['AI', 'English practice'])], 'response_metadata': [{'id': 'call_umGySnuvUTY4fJ9SY2iVLLC7', 'json_doc_id': 'UserProfile'}], 'attempts': 1}\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"I like to study AI and pratice english in my free time\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f21412-c0d1-4722-a620-2a99a00125ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38c296-4ac2-4596-91d2-03bd2ea153ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7da58-020c-4d4a-894f-0f16579ba792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
